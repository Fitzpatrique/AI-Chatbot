{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "CB Project.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Fitzpatrique/AI-Chatbot/blob/main/CB_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dk39s7E67YgF"
      },
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "import numpy as np\n",
        "import re\n",
        "from nltk.stem import wordnet # to perform lemmitization\n",
        "#from sklearn.feature_extraction.text import CountVectorizer # to perform bow\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer # to perform tfidf\n",
        "from nltk import pos_tag # for parts of speech\n",
        "from sklearn.metrics import pairwise_distances # to perform cosine similarity\n",
        "from nltk import word_tokenize #  to create tokens\n",
        "#from nltk.corpus import stopwords # for stop words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sf2wlHq37YgR"
      },
      "source": [
        "df = pd.read_excel('C:\\\\Users\\\\USEER\\\\Desktop\\\\Patrick\\\\Project\\\\CB\\\\dialog_talk_agent.xlsx')\n",
        "#df.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vGUWDJJ57Ygb"
      },
      "source": [
        "df.ffill(axis = 0, inplace = True) # fills the null value with the previous value.\n",
        "#df.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cDP3rkDK7Ygo"
      },
      "source": [
        "def text_normalization(text):\n",
        "    text = str(text).lower() # text to lower case\n",
        "    spl_char_text = re.sub(r'[^a-z]',' ',text) # removing special characters\n",
        "    tokens = nltk.word_tokenize(spl_char_text) # word tokenizing\n",
        "    lema = wordnet.WordNetLemmatizer() # initializing lemmatization\n",
        "    tags_list = pos_tag(tokens, tagset = None) # parts of speech\n",
        "    lema_words = [] # empty list\n",
        "    \n",
        "    for token,pos_token in tags_list:\n",
        "        if pos_token.startswith('V'): # verb\n",
        "            pos_val = 'v'\n",
        "        elif pos_token.startswith('J'): # adjective\n",
        "            pos_val = 'a'\n",
        "        elif pos_token.startswith('R'): # adverb\n",
        "            pos_val = 'r'\n",
        "        else:\n",
        "            pos_val = 'n' # noun\n",
        "            \n",
        "        lema_token = lema.lemmatize(token, pos_val) # performing lemmmatization\n",
        "        lema_words.append(lema_token) # appending lemmatized token into a list\n",
        "    \n",
        "    return \" \".join(lema_words) # returns the lemmatized tokens as a sentence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SbO2e-qr7Ygz",
        "outputId": "9196cabb-a1b2-4059-c173-9982b7113c92"
      },
      "source": [
        "text_normalization('telling you stuff about me')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'tell you stuff about me'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r5G-Wdhy7YhA"
      },
      "source": [
        "df['lemmatized_text'] = df['Context'].apply(text_normalization) # applying the function to dataset to get clean text\n",
        "#df.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0U-NyX7A7YhI"
      },
      "source": [
        "# using tf-idf\n",
        "\n",
        "tfidf = TfidfVectorizer() # initializing tf-idf\n",
        "x_tfidf = tfidf.fit_transform(df['lemmatized_text']).toarray() # transforming the data to array"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9qwnQYYJ7YhU"
      },
      "source": [
        "df_tfidf = pd.DataFrame(x_tfidf, columns=tfidf.get_feature_names())\n",
        "#df_tfidf.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OW2lKb_W7Yhe"
      },
      "source": [
        "def response(text):\n",
        "    lemma = text_normalization(text) # calling the function to perform text normalization\n",
        "    tf = tfidf.transform([lemma]).toarray() # applying tf-idf\n",
        "    cos = 1 - pairwise_distances(df_tfidf,tf,metric='cosine') # applying cosing similarity\n",
        "    index_value = cos.argmax() # getting the index value\n",
        "    return df['Text Response'].loc[index_value]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-gKxlFcT7Yhm"
      },
      "source": [
        "def run():\n",
        "    while True:\n",
        "        text = str(input(\"Enter the 'q' key to exit conversation: \"))\n",
        "        if text == 'q':\n",
        "            print(\"Bye!\")\n",
        "            return False\n",
        "        else:\n",
        "            print(response(text))\n",
        "            return run()\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sW4NPS4_7Yhx",
        "outputId": "d0c58a69-d452-468f-80e6-34697303fa75"
      },
      "source": [
        "run()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enter the 'q' key to exit conversation: hey\n",
            "Hi there, friend!\n",
            "Enter the 'q' key to exit conversation: how are you\n",
            "Lovely, thanks.\n",
            "Enter the 'q' key to exit conversation: fuck you too\n",
            "Thanks! The feeling is mutual.\n",
            "Enter the 'q' key to exit conversation: and your mamsa\n",
            "Lovely, thanks.\n",
            "Enter the 'q' key to exit conversation: bye\n",
            "See you soon!\n",
            "Enter the 'q' key to exit conversation: q\n",
            "Bye!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lLuNnfMC7Yh-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}